## Bayesian machine learning notebooks

This repository is a collection of notebooks about various topics of *Bayesian methods for machine learning*. The links in
the following list display the notebooks of this repository via [nbviewer](https://nbviewer.jupyter.org/) 
to ensure a proper rendering of formulas.


- [Latent variable models - part 1: Gaussian mixture models and the EM algorithm](https://nbviewer.jupyter.org/github/krasserm/bayesian-machine-learning/blob/master/latent_variable_models_part_1.ipynb).
  Introduction to the expectation maximization (EM) algorithm and its application to Gaussian mixture models. Example
  implementation with plain NumPy/SciPy and scikit-learn for comparison.

- Latent variable models - part 2: Approximate inference and variational autoencoders. Work in progress, going to replace 
  [previous work](variational_autoencoder.ipynb) on variational autoencoders.

- [Variational inference in Bayesian neural networks](https://nbviewer.jupyter.org/github/krasserm/bayesian-machine-learning/blob/master/bayesian_neural_networks.ipynb). Demonstrates how to 
  implement and train a Bayesian neural network using a variational inference approach. Example implementation with Keras. 

- [Bayesian regression with linear basis function models](https://nbviewer.jupyter.org/github/krasserm/bayesian-machine-learning/blob/master/bayesian_linear_regression.ipynb). Introduction to Bayesian
  linear regression. Implementation from scratch with plain NumPy as well as usage of scikit-learn for comparison.

- [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/krasserm/bayesian-machine-learning/blob/master/gaussian_processes.ipynb)
  [Gaussian processes](https://nbviewer.jupyter.org/github/krasserm/bayesian-machine-learning/blob/master/gaussian_processes.ipynb). 
  Introduction to Gaussian processes. Example implementations with plain NumPy/SciPy as well as with libraries 
  scikit-learn and GPy. 

- [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/krasserm/bayesian-machine-learning/blob/master/bayesian_optimization.ipynb)
  [Bayesian optimization](https://nbviewer.jupyter.org/github/krasserm/bayesian-machine-learning/blob/master/bayesian_optimization.ipynb). 
  Introduction to Bayesian optimization. Example implementations with plain NumPy/SciPy as well as with libraries 
  scikit-optimize and GPyOpt. Hyperparameter tuning as application example.  

- [Variational auto-encoder](variational_autoencoder.ipynb). About to be replaced by two newer notebooks on latent variable 
  models, [part 1](https://nbviewer.jupyter.org/github/krasserm/bayesian-machine-learning/blob/master/latent_variable_models_part_1.ipynb) and part 2 (work in progress). 

- [Deep feature consistent variational auto-encoder](https://nbviewer.jupyter.org/github/krasserm/bayesian-machine-learning/blob/master/variational_autoencoder_dfc.ipynb). 
  Describes how a perceptual loss can improve the quality of images generated by a variational auto-encoder. Example 
  implementation with Keras.  

- [Conditional generation via Bayesian optimization in latent space](https://nbviewer.jupyter.org/github/krasserm/bayesian-machine-learning/blob/master/variational_autoencoder_opt.ipynb). 
  Describes an approach for conditionally generating outputs with desired properties by doing Bayesian optimization in 
  latent space of variational auto-encoders. Example application implemented with Keras and GPyOpt.

- [Topic modeling with PyMC3](https://nbviewer.jupyter.org/github/krasserm/bayesian-machine-learning/blob/master/topic_modeling_pymc3.ipynb). 
  An introduction to topic models and their implementation with the probabilistic programming library [PyMC3](https://docs.pymc.io/).
