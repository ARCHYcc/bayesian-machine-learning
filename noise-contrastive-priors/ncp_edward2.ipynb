{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import edward2 as ed\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, LeakyReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import L2\n",
    "\n",
    "from utils import (train,\n",
    "                   backprop,\n",
    "                   select_bands, \n",
    "                   select_subset,\n",
    "                   style,\n",
    "                   plot_data, \n",
    "                   plot_prediction, \n",
    "                   plot_uncertainty)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(123)\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"Sinusoidal function.\"\"\"\n",
    "    return 0.5 * np.sin(25 * x) + 0.5 * x\n",
    "\n",
    "\n",
    "def noise(x, slope, rng=np.random):\n",
    "    \"\"\"Create heteroskedastic noise.\"\"\"\n",
    "    noise_std = np.maximum(0.0, x + 1.0) * slope\n",
    "    return rng.normal(0, noise_std).astype(np.float32)\n",
    "\n",
    "x = np.linspace(-1.0, 1.0, 1000, dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "# Noisy samples from f (with heteroskedastic noise)\n",
    "y = f(x) + noise(x, slope=0.2, rng=rng)\n",
    "\n",
    "# Select data from 2 of 5 bands (regions)\n",
    "x_bands, y_bands = select_bands(x, y, mask=[False, True, False, True, False])\n",
    "\n",
    "# Select 40 random samples from these regions\n",
    "x_train, y_train = select_subset(x_bands, y_bands, num=40, rng=rng)\n",
    "\n",
    "plot_data(x_train, y_train, x, f(x))\n",
    "plt.scatter(x, y, **style['bg_data'], label='Noisy data')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = x_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n_hidden=200):\n",
    "    leaky_relu = LeakyReLU(alpha=0.2)\n",
    "    \n",
    "    x_in = Input(shape=(1,))\n",
    "    x = ed.layers.NCPNormalPerturb(stddev=0.5)(x_in)  # double input batch\n",
    "    x = Dense(n_hidden, activation=leaky_relu)(x)\n",
    "    x = Dense(n_hidden, activation=leaky_relu)(x)\n",
    "\n",
    "    means = ed.layers.DenseVariationalDropout(1, activation=None)(x)  # get mean\n",
    "    means = ed.layers.NCPNormalOutput(mean=y_train, stddev=1.0)(means)  # halve input batch\n",
    "    \n",
    "    stddevs = tf.keras.layers.Dense(1, activation='softplus')(x[:batch_size])\n",
    "    outputs = tf.keras.layers.Lambda(lambda x: ed.Normal(x[0], x[1]))([means, stddevs])\n",
    "\n",
    "    return Model(x_in, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "\n",
    "optimizer = tf.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "loss_tracker = tf.keras.metrics.Mean(name='loss')\n",
    "mse_tracker = tf.keras.metrics.MeanSquaredError(name='mse')\n",
    "\n",
    "epochs = 15000\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(x_train)\n",
    "        loss = -tf.reduce_mean(predictions.distribution.log_prob(y_train))\n",
    "        loss += 0.1 * model.losses[0] / batch_size  # KL regularizer for output layer\n",
    "        loss += 0.1 * model.losses[-1]\n",
    "        loss_tracker.update_state(loss)\n",
    "        \n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'epoch = {epoch}, loss = {loss_tracker.result()}')\n",
    "        loss_tracker.reset_states()\n",
    "        \n",
    "    trainable_vars = model.trainable_variables\n",
    "    gradients = tape.gradient(loss, trainable_vars)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.sort(x_train, axis=0)\n",
    "\n",
    "out_dist = model(x_train)\n",
    "\n",
    "aleatoric_uncertainty=out_dist.distribution.stddev()\n",
    "expected_output = out_dist.distribution.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_data(x_train, y_train, x, f(x))\n",
    "plot_prediction(x_test, \n",
    "                expected_output, \n",
    "                aleatoric_uncertainty=aleatoric_uncertainty)\n",
    "plt.ylim(-2, 2)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_uncertainty(x_test, \n",
    "                 aleatoric_uncertainty=aleatoric_uncertainty)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
